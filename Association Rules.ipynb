{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e05e486d-873c-432d-aa83-65d3ee7b6c4c",
   "metadata": {},
   "source": [
    "# -------------- Association Rules ----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9e78afda-fd3d-43c6-b6ee-2f4f1b314a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990dcfb7-ac74-4dd3-833c-7a964e4d5fdf",
   "metadata": {},
   "source": [
    "## --- 1. DATA LOADING ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "61a43cda-d587-4d42-976e-522e4083f648",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>shrimp,almonds,avocado,vegetables mix,green gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>burgers,meatballs,eggs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chutney</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>turkey,avocado</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mineral water,milk,energy bar,whole wheat rice...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  shrimp,almonds,avocado,vegetables mix,green gr...\n",
       "1                             burgers,meatballs,eggs\n",
       "2                                            chutney\n",
       "3                                     turkey,avocado\n",
       "4  mineral water,milk,energy bar,whole wheat rice..."
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_excel('Online retail.xlsx',sheet_name='Sheet1',header=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7eafe2-e4aa-42b6-8202-8f30751ac220",
   "metadata": {},
   "source": [
    "## --- 2. DATA PREPROCESSING (Cleaning and Formatting) ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "033a6771-a939-4573-a886-b63baec3510a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7501, 1)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "05482558-cfe3-4e06-8ef4-621038fdad9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7501 entries, 0 to 7500\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   0       7501 non-null   object\n",
      "dtypes: object(1)\n",
      "memory usage: 58.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9f5d4579-0d0f-4c4d-b306-2bba26d1bf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the single column (which contains the entire transaction string)\n",
    "transactions_series = df.iloc[:, 0].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "41afaa12-fc7b-4f75-b357-5920e34fbd4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 3 Parsed Transactions:\n",
      "[['shrimp', 'almonds', 'avocado', 'vegetables mix', 'green grapes', 'whole weat flour', 'yams', 'cottage cheese', 'energy drink', 'tomato juice', 'low fat yogurt', 'green tea', 'honey', 'salad', 'mineral water', 'salmon', 'antioxydant juice', 'frozen smoothie', 'spinach', 'olive oil'], ['burgers', 'meatballs', 'eggs'], ['chutney']]\n"
     ]
    }
   ],
   "source": [
    "# Convert each transaction string into a list of individual items\n",
    "# 1. Strip quotes and leading/trailing spaces\n",
    "# 2. Split the string by the comma (',')\n",
    "# 3. Filter out any empty strings that may result from extra spaces\n",
    "transactions = []\n",
    "for transaction_str in transactions_series:\n",
    "    # Handle both double quotes and standard CSV formatting\n",
    "    cleaned_str = transaction_str.strip('\\\"').strip()\n",
    "    items = [item.strip() for item in cleaned_str.split(',') if item.strip()]\n",
    "    if items:\n",
    "        transactions.append(items)\n",
    "\n",
    "print(\"First 3 Parsed Transactions:\")\n",
    "print(transactions[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a8695fc4-a3ab-4878-a2ab-be4e3bc6ae20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "One-Hot Encoded Basket Head:\n",
      "   almonds  antioxydant juice  asparagus  avocado  babies food  bacon  \\\n",
      "0     True               True      False     True        False  False   \n",
      "1    False              False      False    False        False  False   \n",
      "2    False              False      False    False        False  False   \n",
      "3    False              False      False     True        False  False   \n",
      "4    False              False      False    False        False  False   \n",
      "\n",
      "   barbecue sauce  black tea  blueberries  body spray  ...  turkey  \\\n",
      "0           False      False        False       False  ...   False   \n",
      "1           False      False        False       False  ...   False   \n",
      "2           False      False        False       False  ...   False   \n",
      "3           False      False        False       False  ...    True   \n",
      "4           False      False        False       False  ...   False   \n",
      "\n",
      "   vegetables mix  water spray  white wine  whole weat flour  \\\n",
      "0            True        False       False              True   \n",
      "1           False        False       False             False   \n",
      "2           False        False       False             False   \n",
      "3           False        False       False             False   \n",
      "4           False        False       False             False   \n",
      "\n",
      "   whole wheat pasta  whole wheat rice   yams  yogurt cake  zucchini  \n",
      "0              False             False   True        False     False  \n",
      "1              False             False  False        False     False  \n",
      "2              False             False  False        False     False  \n",
      "3              False             False  False        False     False  \n",
      "4              False              True  False        False     False  \n",
      "\n",
      "[5 rows x 119 columns]\n"
     ]
    }
   ],
   "source": [
    "## Convert the list of lists (transactions) into the required DataFrame format (One-Hot Encoded)\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(transactions).transform(transactions)\n",
    "basket_sets = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "print(\"\\nOne-Hot Encoded Basket Head:\")\n",
    "print(basket_sets.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cc66de-2f9d-41b3-8b9a-97a968463ef8",
   "metadata": {},
   "source": [
    "## --- 3. Association Rule Mining (Apriori) ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a3422320-572d-4d5c-ada2-02c0ffbf8554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Frequent Itemsets (min_support=0.01, Top 10):\n",
      "    support          itemsets\n",
      "0  0.020397         (almonds)\n",
      "1  0.033329         (avocado)\n",
      "2  0.010799  (barbecue sauce)\n",
      "3  0.014265       (black tea)\n",
      "4  0.011465      (body spray)\n",
      "5  0.033729        (brownies)\n",
      "6  0.087188         (burgers)\n",
      "7  0.030129          (butter)\n",
      "8  0.081056            (cake)\n",
      "9  0.015331         (carrots)\n"
     ]
    }
   ],
   "source": [
    "## 3.1 Run Apriori Algorithm to Find Frequent Itemsets\n",
    "# Start with a suitable minimum support (e.g., 0.01 or 1% of transactions)\n",
    "min_support = 0.01\n",
    "frequent_itemsets = apriori(basket_sets, min_support=min_support, use_colnames=True)\n",
    "\n",
    "print(f\"\\nFrequent Itemsets (min_support={min_support}, Top 10):\")\n",
    "print(frequent_itemsets.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b4870032-f6b8-43d2-9246-261eb6d09bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3.2 Generate Association Rules\n",
    "# Generate rules using lift as the primary metric, requiring it to be > 1.\n",
    "rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25beb5a-37a3-42ba-8c9e-aedfdbe01bc7",
   "metadata": {},
   "source": [
    "## --- 4. Analysis and Interpretation ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f8d1347b-9dbe-4423-9f3e-98fb7c7879f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Meaningful Rules (Lift >= 1.2, Confidence >= 0.6):\n",
      "Empty DataFrame\n",
      "Columns: [antecedents, consequents, antecedent support, consequent support, support, confidence, lift, representativity, leverage, conviction, zhangs_metric, jaccard, certainty, kulczynski]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "## 4.1 Set Appropriate Thresholds for Meaningful Rules\n",
    "\n",
    "# Filter for rules that are strong (high confidence) AND interesting (high lift)\n",
    "meaningful_rules = rules[ (rules['lift'] >= 1.2) &\n",
    "                          (rules['confidence'] >= 0.6) ]\n",
    "\n",
    "# Sort the rules by Lift (descending) to find the strongest relationships.\n",
    "meaningful_rules = meaningful_rules.sort_values(by='lift', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"\\nMeaningful Rules (Lift >= 1.2, Confidence >= 0.6):\")\n",
    "print(meaningful_rules.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a114c50a-b898-4c33-8a1e-e7ba8ec5ce3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No rules met the stringent thresholds (Lift >= 1.2 and Confidence >= 0.6). Try lowering the minimum support or minimum confidence to extract more rules.\n"
     ]
    }
   ],
   "source": [
    "## 4.2 Interpretation and Insights\n",
    "\n",
    "if not meaningful_rules.empty:\n",
    "    top_rule = meaningful_rules.iloc[0]\n",
    "    antecedents = set(top_rule['antecedents'])\n",
    "    consequents = set(top_rule['consequents'])\n",
    "    confidence = top_rule['confidence'] * 100\n",
    "    lift = top_rule['lift']\n",
    "\n",
    "    print(f\"\\n--- Customer Purchasing Insight (Top Rule) ---\")\n",
    "    print(f\"Rule: {antecedents} -> {consequents}\")\n",
    "    print(f\"Confidence: {confidence:.2f}% | Lift: {lift:.2f}\")\n",
    "\n",
    "    print(f\"\\nInsight: Customers who purchase {antecedents} have a **{confidence:.2f}%** chance of also buying {consequents}. The **Lift of {lift:.2f}** shows this co-occurrence is **{lift:.2f} times** more frequent than pure chance, indicating a strong complementary or habitual purchase pattern. This insight is useful for optimizing store layout or creating bundled promotions.\")\n",
    "else:\n",
    "    print(\"\\nNo rules met the stringent thresholds (Lift >= 1.2 and Confidence >= 0.6). Try lowering the minimum support or minimum confidence to extract more rules.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc53f01-4ffe-4aee-8b38-f61fe958be61",
   "metadata": {},
   "source": [
    "# Interview Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba99aa3e-3b40-4735-bb6a-178bc5866553",
   "metadata": {},
   "source": [
    "## 1. What is Lift and why is it important in Association rules?\n",
    "Lift measures how much more likely the antecedent (item A) and consequent (item B) are to be purchased together than would be expected if they were independent purchases.\n",
    "\n",
    "Lift= \n",
    "Support (B)\n",
    "Confidence (A→B)\n",
    "​\n",
    " = \n",
    "P(A)×P(B)\n",
    "P(A∩B)\n",
    "​\n",
    " \n",
    "**Importance of Lift:**\n",
    "**Identifies True Relationships:** Lift helps differentiate between truly related items and items that are simply popular. A rule might have high support and confidence just because the consequent (B) is bought frequently by everyone, regardless of whether the antecedent (A) is present.\n",
    "\n",
    "**Measures Strength:**\n",
    "\n",
    "Lift = 1: The occurrence of A and B is independent. The rule is no better than chance.\n",
    "\n",
    "Lift > 1: A and B are positively correlated (bought together more often than expected). This indicates a useful rule for cross-selling.\n",
    "\n",
    "Lift < 1: A and B are negatively correlated (one discourages the purchase of the other).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a88a83-587c-43ff-a9f6-69160cd91dec",
   "metadata": {},
   "source": [
    "## 2. What are Support and Confidence? How do you calculate them?\n",
    "Support\n",
    "Support is an indication of how frequently the itemset appears in the dataset. It is the proportion of total transactions that contain the specified itemset.\n",
    "\n",
    "**Calculation:**\n",
    "\n",
    "Support (A)= \n",
    "Total number of transactions\n",
    "Number of transactions containing A\n",
    "​\n",
    " \n",
    "$$$$For an itemset {A,B}:\n",
    "\n",
    "Support (A∪B)= \n",
    "Total number of transactions\n",
    "Number of transactions containing both A and B\n",
    "​\n",
    " \n",
    "$$$$\n",
    "\n",
    "**Role:** Used in the Apriori algorithm to filter out infrequent itemsets early on, saving computation time.\n",
    "\n",
    "**Confidence**\n",
    "Confidence is a measure of the reliability of the rule. It is the conditional probability that the consequent (B) will be purchased given that the antecedent (A) has been purchased.\n",
    "\n",
    "**Calculation:**\n",
    "\n",
    "Confidence (A→B)= \n",
    "Support (A)\n",
    "Support (A∪B)\n",
    "​\n",
    " \n",
    "$$$$\n",
    "\n",
    "**Role:** Indicates the strength of the association in one direction (A→B). A high confidence suggests that the rule is likely to hold true in future transactions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9996830c-cbfc-49f3-bbe4-1d81d0da8694",
   "metadata": {},
   "source": [
    "## 3. What are some limitations or challenges of Association rules mining?\n",
    "The primary limitations and challenges of Association Rule Mining, particularly with the Apriori algorithm, fall into three main categories: scalability, threshold sensitivity, and data restriction. The biggest hurdle is computational scalability, known as the combinatorial explosion problem. As the number of unique items (N) in a dataset increases (common in large retail), the number of potential itemsets grows exponentially (2 \n",
    "N\n",
    " ). This makes finding all frequent itemsets extremely time- and memory-intensive. Second, the quality of the rules is highly sensitive to the arbitrary selection of thresholds for minimum Support and Confidence. A threshold that is set too low generates a massive number of rules, many of which are trivial or obvious (e.g., \"If someone buys a steering wheel, they buy a car\"), burying the useful insights. Conversely, a threshold set too high might filter out rare but potentially highly profitable relationships. Finally, the technique is fundamentally designed for binary data (an item is present or absent in a transaction). Handling quantitative information (e.g., the specific quantity or price of an item) requires additional, complex preprocessing steps like discretization, which can lead to a loss of valuable data and context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b48f98a-5bb0-40ff-8e44-82c5eb9b6c94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
